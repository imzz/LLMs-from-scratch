# Chapter 3: Coding Attention Mechanisms

### Main Chapter Code

- [ch03.ipynb](ch03.ipynb) contains all the code as it appears in the chapter
- 3.1 长序列建模的问题
本节没有代码，主要讨论了在处理长序列时的问题。例如，逐字翻译文本是不可行的，因为源语言和目标语言的语法结构不同。在Transformer模型出现之前，机器翻译任务通常使用编码器-解码器RNN（循环神经网络）。在这种架构中，编码器处理源语言的序列，生成一个隐藏状态（即神经网络的中间层），用于表示整个输入序列的压缩信息。

3.2 使用注意力机制捕捉数据依赖关系
本节也没有代码，主要介绍了注意力机制的作用。通过注意力机制，文本生成的解码器部分可以选择性地访问所有输入标记，这意味着某些输入标记在生成特定输出标记时比其他标记更重要。自注意力机制（Self-Attention）是Transformer模型中的一种技术，它通过让序列中的每个位置与其他位置交互，来增强输入表示。

3.3 使用自注意力机制关注输入的不同部分
3.3.1 没有可训练权重的简单自注意力机制
本节介绍了一个非常简化的自注意力机制，它没有任何可训练权重，仅用于说明目的。这个机制并不是Transformer中使用的真实注意力机制，下一节（3.3.2）会扩展这个简单机制，实现真正的自注意力机制。

假设我们有一个输入序列，每个输入是一个文本（例如句子“Your journey starts with one step”），并且已经将其转换为词嵌入（如第2章所述）。目标是计算每个输入元素的上下文向量，该向量是输入序列的加权和，权重由注意力机制决定。

3.3.2 计算所有输入标记的注意力权重
本节将前面的计算推广到所有输入标记，计算所有注意力权重和上下文向量。在自注意力机制中，首先计算注意力分数，然后通过Softmax函数将其归一化为注意力权重（总和为1），最后通过加权求和生成上下文向量。

3.4 实现带有可训练权重的自注意力机制
本节实现了Transformer架构中使用的自注意力机制，称为缩放点积注意力（Scaled Dot-Product Attention）。与之前的简单机制相比，最大的区别是引入了可训练的权重矩阵，这些矩阵在模型训练过程中会被更新。

3.4.1 逐步计算注意力权重
本节逐步实现了自注意力机制，引入了三个可训练的权重矩阵：查询矩阵（Query）、键矩阵（Key）和值矩阵（Value）。通过这些矩阵，输入嵌入被投影为查询向量、键向量和值向量。

3.4.2 实现一个紧凑的自注意力类
本节将前面的步骤整合到一个SelfAttention类中，并使用PyTorch的Linear层来简化实现。

3.5 使用因果注意力机制隐藏未来词
在因果注意力机制中，注意力权重矩阵的上三角部分被掩码，确保模型在计算上下文向量时无法使用未来的标记。

3.5.1 应用因果注意力掩码
本节将前面的自注意力机制转换为因果自注意力机制，确保模型在预测某个位置的输出时，只能依赖于该位置之前的已知输出。

3.5.2 使用Dropout掩码额外的注意力权重
为了减少过拟合，本节在计算注意力权重后应用了Dropout，随机丢弃一部分注意力权重。

3.5.3 实现一个紧凑的因果自注意力类
本节实现了一个完整的因果自注意力类，支持批量输入，并集成了Dropout和因果掩码。

3.6 将单头注意力扩展到多头注意力
3.6.1 堆叠多个单头注意力层
本节通过堆叠多个单头注意力模块来实现多头注意力机制。多头注意力的主要思想是并行运行多个注意力机制，每个机制使用不同的线性投影，从而让模型能够从不同的表示子空间中联合关注信息。

3.6.2 使用权重分割实现多头注意力
本节实现了一个独立的MultiHeadAttention类，通过将查询、键和值矩阵分割为多个头来实现多头注意力。

解释：
这段内容详细介绍了如何从零开始实现注意力机制，特别是自注意力机制和多头注意力机制。注意力机制是Transformer模型的核心，它帮助模型在处理文本等序列数据时，能够动态地关注序列中不同部分的重要性。

自注意力机制：通过计算输入序列中每个元素与其他元素的相关性（注意力分数），生成上下文向量。这些上下文向量能够捕捉序列中不同部分的关系。

多头注意力机制：通过并行运行多个自注意力机制，每个机制关注不同的表示子空间，从而增强模型的表达能力。

因果注意力机制：在生成文本时，确保模型只能看到当前时刻之前的输入，而不能看到未来的输入。这是通过掩码实现的。

Dropout：在训练过程中，随机丢弃一部分注意力权重，以防止模型过拟合。

通过这些机制，模型能够更好地理解和生成文本，这也是现代大型语言模型（如GPT）的核心技术之一。

总结：
这段内容从简单的自注意力机制开始，逐步扩展到多头注意力和因果注意力机制，最终实现了一个完整的注意力模块。这些模块是构建大型语言模型的基础，能够帮助模型更好地理解和生成文本。

### Optional Code

- [multihead-attention.ipynb](multihead-attention.ipynb) is a minimal notebook with the main data loading pipeline implemented in this chapter

